{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_inputs_test as gen, adam, initialize_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_init(nx : int, seed : int = None):\n",
    "\n",
    "    # generate the sample random numbers \n",
    "    if seed : np.random.seed(seed=seed)\n",
    "    # initializing params as python dictionary \n",
    "    params = {}\n",
    "\n",
    "    # creating weight and bias for one neuron\n",
    "    # as we said to the README file we just have one neuron (binary classification)\n",
    "    ny = 1\n",
    "    for i in range(ny):\n",
    "        params[f'W{i+1}'] = np.random.randn(1, nx) * 0.01\n",
    "        params[f'b{i+1}'] = np.zeros((1, 1))\n",
    "\n",
    "    # returning the output as a dictionary\n",
    "    return params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x : int):\n",
    "    # computing sigmoid function for a given value x \n",
    "\n",
    "    sig = 1. / (1 + np.exp(-x))\n",
    "\n",
    "    return sig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X : np.ndarray, params : dict):\n",
    "    # layers \n",
    "    layers = len(list(params.keys())) // 2\n",
    "\n",
    "    # linearity & prediction \n",
    "    # Z = W.dot(X) + b\n",
    "    # A = sigmoid(Z)\n",
    "    Z, A = {},{}\n",
    "  \n",
    "    for layer in range(layers):\n",
    "        # linearity\n",
    "        Z[layer+1] = params[f\"W{layer+1}\"].dot(X) + params[f'b{layer+1}']\n",
    "        # prediction\n",
    "        A[layer+1] = sigmoid(x=Z[layer+1])\n",
    "    \n",
    "    cache = {\"Z\" : Z, \"A\" : A}\n",
    "\n",
    "    return A[layer+1], cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(A : np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    * A -- The activation output of the last layer, of shape (sy, m)\n",
    "    * Y -- \"true\" labels vector of shape (ny, m)\n",
    "\n",
    "    in the case of binary classification ny = 1\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    A.shape = \n",
    "    Y.shape = (1, ny)\n",
    "    \"\"\"\n",
    "    \n",
    "    m       = Y.shape[1] \n",
    "    cost    = (1./m) * (-np.dot(Y, np.log(A).T) - np.dot(1-Y, np.log(1-A).T))\n",
    "    # np.squeeze(cost) --> np.squeeze([[1]]) = 1.0\n",
    "    cost    = float(np.squeeze(cost))   \n",
    "                                    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(\n",
    "        params  : dict, \n",
    "        cache   : dict, \n",
    "        X       : np.ndarray, \n",
    "        Y       : np.ndarray\n",
    "        ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing A, and Z (activations and linear calculations).\n",
    "    X -- input data of shape (nx, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (ny, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    # nomber of sample \n",
    "    m       = X.shape[1]\n",
    "    # intialize gradient \n",
    "    grads   = {}\n",
    "    # restoring activation & linearity from cache \n",
    "    A, Z    = cache['A'], cache['Z']  \n",
    "    # computing the number of layers \n",
    "    layers  = len(list(params.keys())) // 2\n",
    "\n",
    "    # updating gradient \n",
    "    # dZ = A[layers] - Y\n",
    "    grads[f'dZ{layers}'] = A[layers] - Y\n",
    "    # dW = dZ.dot(X.T)\n",
    "    grads[f'dW{layers}'] = grads[f'dZ{layers}'].dot( X.T) / m\n",
    "    # db = 1\n",
    "    grads[f'db{layers}'] = np.sum(grads[f'dZ{layers}'], axis=1, keepdims=True) / m\n",
    "\n",
    "    return grads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients : dict = {}, maxValue : float = 10):\n",
    "\n",
    "    \"\"\"\n",
    "    clip funsion is ued to solve the issue of exploding gradient.\n",
    "    by fixing the border limits the gradient cannot exceded we can solve that problem \n",
    "\n",
    "    * firstly we start by defining the border limits [-L, L]\n",
    "    if the value of gardient id outside of this range we put it inside like this \n",
    "        - if grad > L  ---> grad = L\n",
    "        - if grad < -L ---> grad = L\n",
    "        - else grad = pass\n",
    "\n",
    "    * so to make it possible we need the value of L in this case L = maxValues argument\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, value in gradients.items():\n",
    "        np.clip(a=value, a_min=-maxValue, a_max=maxValue, out=value)\n",
    "\n",
    "        gradients[key] = value\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(A, as_bool : bool = True):\n",
    "    \n",
    "    prediction =  A > 0.5\n",
    "    \n",
    "    if as_bool is True : return prediction * 1\n",
    "    else: return A\n",
    "\n",
    "def scoring(y_true : np.ndarray, y_pred : np.ndarray):\n",
    "\n",
    "    return (y_pred == y_true).sum() / y_pred.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updating_params(grads : dict, params : dict, learning_rate : float = 0.01):\n",
    "\n",
    "    # number of layers\n",
    "    layers = len( list(params.keys()) ) // 2\n",
    "\n",
    "    # updating the weight and bias using the gradient (stochastic gradient descend)\n",
    "    for layer in range(layers):\n",
    "        params[f'W{layer+1}'] = params[f'W{layer+1}'] - learning_rate * grads[f'dW{layer+1}']\n",
    "        params[f'b{layer+1}'] = params[f'b{layer+1}'] - learning_rate * grads[f'db{layer+1}']\n",
    "\n",
    "    # returning the updating weight and bias \n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(\n",
    "        X : np.ndarray,\n",
    "        Y : np.ndarray,\n",
    "        maxValue : float = None,\n",
    "        early_stoping       : int   = 10000,\n",
    "        learning_rate       : float = 1e-3,\n",
    "        print_val           : bool  = True,\n",
    "        epsilon             : float = 1e-8,\n",
    "        verbose             : bool  = True,\n",
    "        step                : int   = 100, \n",
    "        seed                : int   = None,\n",
    "        optimizer           : str   = \"sgd\",\n",
    "        beta1               : float = 0.8,\n",
    "        beta2               : float = 0.85,\n",
    "        t                   : float = 2.5\n",
    "       ):\n",
    "\n",
    "    nx      = X.shape[0]\n",
    "    params  = params_init(nx=nx, seed=seed)\n",
    "\n",
    "    history = {\"cost\" : [], \"accuracy\" : []}\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while True:\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\"\n",
    "        A_last, cache = forward_propagation(X=X, params=params)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n",
    "        # cost = ...\n",
    "        cost = cost_function(A=A_last, Y=Y)\n",
    "        \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(params=params, cache=cache, X=X, Y=Y)\n",
    "        \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        if maxValue : grads = clip(gradients=grads, maxValue=maxValue)\n",
    "\n",
    "        if optimizer == \"sgd\" : params = updating_params(params=params, grads=grads, learning_rate=learning_rate)\n",
    "        else:\n",
    "            v, s = initialize_adam(params=params) \n",
    "            params,v, s = adam(params=params, grads=grads, v=v, s=s, t=t, beta1=beta1, beta2=beta2, learning_rate=learning_rate, epsilon=epsilon)\n",
    "           \n",
    "        y_pred      = prediction(A_last, as_bool=True)\n",
    "        accuracy    = scoring(y_true=Y, y_pred=y_pred)\n",
    "                        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if verbose and i % step == 0:        \n",
    "            if i == 0 : pass \n",
    "            else:\n",
    "                if history['cost']:\n",
    "                    if np.abs(history['cost'][-1] - cost) < epsilon: \n",
    "                        print(\"loop break due np.abs( cost[i] - cost[i-1]) < epsilon\")\n",
    "                        break\n",
    "                    else: pass\n",
    "                else: pass \n",
    "\n",
    "                history['cost'].append(cost)\n",
    "                history['accuracy'].append(accuracy)\n",
    "\n",
    "                if print_val : print (\"epoch: %i cost: %f scoring: %f\" %(i, cost, accuracy))\n",
    "        else: pass \n",
    "        \n",
    "        i += 1 \n",
    "\n",
    "             \n",
    "        if early_stoping:\n",
    "            if i >= early_stoping: \n",
    "                print(\"loop break due to early stoping parameter\")\n",
    "                break \n",
    "            else: pass \n",
    "        else: pass\n",
    "        \n",
    "    return history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 8\n",
    "X, y = gen(nx=nx, samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 1900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 2900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 3900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 4900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 5900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 6900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 7900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 8900 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9000 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9100 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9200 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9300 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9400 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9500 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9600 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9700 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9800 cost: 0.616652 scoring: 0.693300\n",
      "epoch: 9900 cost: 0.616652 scoring: 0.693300\n",
      "loop break due to early stoping parameter\n"
     ]
    }
   ],
   "source": [
    "history = LogisticRegression(X=X, Y=y, maxValue=None, print_val=True, learning_rate=0.1, beta1=0.9, beta2=0.99,\n",
    "                                                step=100, early_stoping=10000, epsilon=1e-17, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = 'https://sololearn.com/uploads/files/titanic.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.feature_selection import RFE, VarianceThreshold, SelectPercentile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "data = pd.read_csv(path, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses</th>\n",
       "      <th>Parents/Children</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  Siblings/Spouses  Parents/Children     Fare\n",
       "0         0       3    male  22.0                 1                 0   7.2500\n",
       "1         1       1  female  38.0                 1                 0  71.2833\n",
       "2         1       3  female  26.0                 0                 0   7.9250\n",
       "3         1       1  female  35.0                 1                 0  53.1000\n",
       "4         0       3    male  35.0                 0                 0   8.0500"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 887 entries, 0 to 886\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Survived          887 non-null    int64  \n",
      " 1   Pclass            887 non-null    int64  \n",
      " 2   Sex               887 non-null    object \n",
      " 3   Age               887 non-null    float64\n",
      " 4   Siblings/Spouses  887 non-null    int64  \n",
      " 5   Parents/Children  887 non-null    int64  \n",
      " 6   Fare              887 non-null    float64\n",
      "dtypes: float64(2), int64(4), object(1)\n",
      "memory usage: 48.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 70\n",
      "after 0\n"
     ]
    }
   ],
   "source": [
    "# checking duplicated values\n",
    "duplicated_val = data.duplicated().sum()\n",
    "print(f\"before {duplicated_val}\")\n",
    "\n",
    "# removing duplicated values, i used here inplace to change dataframe directly\n",
    "data.drop_duplicates(inplace=True)\n",
    "print(f\"after {data.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not null values in this dataset\n",
      "Survived            0\n",
      "Pclass              0\n",
      "Sex                 0\n",
      "Age                 0\n",
      "Siblings/Spouses    0\n",
      "Parents/Children    0\n",
      "Fare                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking null values \n",
    "null_vals = data.isna().sum()\n",
    "print(f\"not null values in this dataset\\n{null_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import GridSearchCV as Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses</th>\n",
       "      <th>Parents/Children</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  Siblings/Spouses  Parents/Children     Fare\n",
       "0         0       3    1  22.0                 1                 0   7.2500\n",
       "1         1       1    0  38.0                 1                 0  71.2833\n",
       "2         1       3    0  26.0                 0                 0   7.9250\n",
       "3         1       1    0  35.0                 1                 0  53.1000\n",
       "4         0       3    1  35.0                 0                 0   8.0500"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "data['Sex'] = encoder.fit_transform(data['Sex'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses</th>\n",
       "      <th>Parents/Children</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.332440</td>\n",
       "      <td>-0.519580</td>\n",
       "      <td>-0.068815</td>\n",
       "      <td>-0.061802</td>\n",
       "      <td>0.063134</td>\n",
       "      <td>0.243279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.332440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.112604</td>\n",
       "      <td>-0.387949</td>\n",
       "      <td>0.105392</td>\n",
       "      <td>0.038934</td>\n",
       "      <td>-0.548349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>-0.519580</td>\n",
       "      <td>0.112604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099127</td>\n",
       "      <td>-0.095775</td>\n",
       "      <td>-0.235186</td>\n",
       "      <td>-0.170175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.068815</td>\n",
       "      <td>-0.387949</td>\n",
       "      <td>0.099127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.308451</td>\n",
       "      <td>-0.203584</td>\n",
       "      <td>0.107156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Siblings/Spouses</th>\n",
       "      <td>-0.061802</td>\n",
       "      <td>0.105392</td>\n",
       "      <td>-0.095775</td>\n",
       "      <td>-0.308451</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403822</td>\n",
       "      <td>0.146354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parents/Children</th>\n",
       "      <td>0.063134</td>\n",
       "      <td>0.038934</td>\n",
       "      <td>-0.235186</td>\n",
       "      <td>-0.203584</td>\n",
       "      <td>0.403822</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.203017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.243279</td>\n",
       "      <td>-0.548349</td>\n",
       "      <td>-0.170175</td>\n",
       "      <td>0.107156</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.203017</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Survived    Pclass       Sex       Age  Siblings/Spouses  \\\n",
       "Survived          1.000000 -0.332440 -0.519580 -0.068815         -0.061802   \n",
       "Pclass           -0.332440  1.000000  0.112604 -0.387949          0.105392   \n",
       "Sex              -0.519580  0.112604  1.000000  0.099127         -0.095775   \n",
       "Age              -0.068815 -0.387949  0.099127  1.000000         -0.308451   \n",
       "Siblings/Spouses -0.061802  0.105392 -0.095775 -0.308451          1.000000   \n",
       "Parents/Children  0.063134  0.038934 -0.235186 -0.203584          0.403822   \n",
       "Fare              0.243279 -0.548349 -0.170175  0.107156          0.146354   \n",
       "\n",
       "                  Parents/Children      Fare  \n",
       "Survived                  0.063134  0.243279  \n",
       "Pclass                    0.038934 -0.548349  \n",
       "Sex                      -0.235186 -0.170175  \n",
       "Age                      -0.203584  0.107156  \n",
       "Siblings/Spouses          0.403822  0.146354  \n",
       "Parents/Children          1.000000  0.203017  \n",
       "Fare                      0.203017  1.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = data.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Survived']\n",
    "X = data.drop(labels=['Survived'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amib\\AppData\\Roaming\\Python\\Python311\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\amib\\AppData\\Roaming\\Python\\Python311\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\amib\\AppData\\Roaming\\Python\\Python311\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.countplot(x=target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X, target, test_size=0.2, random_state=3, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = min_max.fit_transform(X_train)\n",
    "X_test  = min_max.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=lr)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y=y_train)\n",
    "X_test_rfe  = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = rfe.get_support()\n",
    "plt.imshow(mask.reshape((1, -1)), cmap=\"gray_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pclass', 'Sex', 'Age'], dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = X.columns[mask]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.1, 1, 2, 5, 10],\n",
       "                         &#x27;solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                    &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.1, 1, 2, 5, 10],\n",
       "                         &#x27;solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                    &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.01, 0.1, 1, 2, 5, 10],\n",
       "                         'solver': ['lbfgs', 'liblinear', 'newton-cg',\n",
       "                                    'newton-cholesky', 'sag', 'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"solver\" : ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    \"C\" : [0.01, 0.1, 1, 2, 5, 10]\n",
    "}\n",
    "gs = Gs(estimator=lr, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "gs.fit(X_train_rfe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=2, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=2, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=2, solver='liblinear')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score test : 0.8109756097560976\n"
     ]
    }
   ],
   "source": [
    "lr = LR(C=2, solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(f\"score test : {lr.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000 cost: 0.685739 scoring: 0.595712\n",
      "epoch: 2000 cost: 0.681472 scoring: 0.595712\n",
      "epoch: 3000 cost: 0.678834 scoring: 0.595712\n",
      "epoch: 4000 cost: 0.677193 scoring: 0.595712\n",
      "epoch: 5000 cost: 0.676164 scoring: 0.595712\n",
      "epoch: 6000 cost: 0.675512 scoring: 0.595712\n",
      "epoch: 7000 cost: 0.675093 scoring: 0.595712\n",
      "epoch: 8000 cost: 0.674819 scoring: 0.595712\n",
      "epoch: 9000 cost: 0.674635 scoring: 0.595712\n",
      "epoch: 10000 cost: 0.674507 scoring: 0.595712\n",
      "epoch: 11000 cost: 0.674415 scoring: 0.595712\n",
      "epoch: 12000 cost: 0.674345 scoring: 0.595712\n",
      "epoch: 13000 cost: 0.674291 scoring: 0.595712\n",
      "epoch: 14000 cost: 0.674245 scoring: 0.595712\n",
      "epoch: 15000 cost: 0.674206 scoring: 0.595712\n",
      "epoch: 16000 cost: 0.674172 scoring: 0.595712\n",
      "epoch: 17000 cost: 0.674141 scoring: 0.595712\n",
      "epoch: 18000 cost: 0.674112 scoring: 0.595712\n",
      "epoch: 19000 cost: 0.674084 scoring: 0.595712\n",
      "epoch: 20000 cost: 0.674059 scoring: 0.595712\n",
      "epoch: 21000 cost: 0.674035 scoring: 0.595712\n",
      "epoch: 22000 cost: 0.674012 scoring: 0.595712\n",
      "epoch: 23000 cost: 0.673989 scoring: 0.595712\n",
      "epoch: 24000 cost: 0.673968 scoring: 0.595712\n",
      "epoch: 25000 cost: 0.673948 scoring: 0.595712\n",
      "epoch: 26000 cost: 0.673929 scoring: 0.595712\n",
      "epoch: 27000 cost: 0.673910 scoring: 0.595712\n",
      "epoch: 28000 cost: 0.673892 scoring: 0.595712\n",
      "epoch: 29000 cost: 0.673875 scoring: 0.595712\n",
      "epoch: 30000 cost: 0.673858 scoring: 0.595712\n",
      "epoch: 31000 cost: 0.673843 scoring: 0.595712\n",
      "epoch: 32000 cost: 0.673827 scoring: 0.595712\n",
      "epoch: 33000 cost: 0.673813 scoring: 0.595712\n",
      "epoch: 34000 cost: 0.673799 scoring: 0.595712\n",
      "epoch: 35000 cost: 0.673785 scoring: 0.595712\n",
      "epoch: 36000 cost: 0.673772 scoring: 0.595712\n",
      "epoch: 37000 cost: 0.673760 scoring: 0.595712\n",
      "epoch: 38000 cost: 0.673748 scoring: 0.595712\n",
      "epoch: 39000 cost: 0.673737 scoring: 0.595712\n",
      "epoch: 40000 cost: 0.673726 scoring: 0.595712\n",
      "epoch: 41000 cost: 0.673715 scoring: 0.595712\n",
      "epoch: 42000 cost: 0.673705 scoring: 0.595712\n",
      "epoch: 43000 cost: 0.673695 scoring: 0.595712\n",
      "epoch: 44000 cost: 0.673686 scoring: 0.595712\n",
      "epoch: 45000 cost: 0.673677 scoring: 0.595712\n",
      "epoch: 46000 cost: 0.673668 scoring: 0.595712\n",
      "epoch: 47000 cost: 0.673660 scoring: 0.595712\n",
      "epoch: 48000 cost: 0.673652 scoring: 0.595712\n",
      "epoch: 49000 cost: 0.673644 scoring: 0.595712\n",
      "epoch: 50000 cost: 0.673637 scoring: 0.595712\n",
      "epoch: 51000 cost: 0.673629 scoring: 0.595712\n",
      "epoch: 52000 cost: 0.673623 scoring: 0.595712\n",
      "epoch: 53000 cost: 0.673616 scoring: 0.595712\n",
      "epoch: 54000 cost: 0.673610 scoring: 0.595712\n",
      "epoch: 55000 cost: 0.673604 scoring: 0.595712\n",
      "epoch: 56000 cost: 0.673598 scoring: 0.595712\n",
      "epoch: 57000 cost: 0.673592 scoring: 0.595712\n",
      "epoch: 58000 cost: 0.673587 scoring: 0.595712\n",
      "epoch: 59000 cost: 0.673582 scoring: 0.595712\n",
      "epoch: 60000 cost: 0.673576 scoring: 0.595712\n",
      "epoch: 61000 cost: 0.673572 scoring: 0.595712\n",
      "epoch: 62000 cost: 0.673567 scoring: 0.595712\n",
      "epoch: 63000 cost: 0.673563 scoring: 0.595712\n",
      "epoch: 64000 cost: 0.673558 scoring: 0.595712\n",
      "epoch: 65000 cost: 0.673554 scoring: 0.595712\n",
      "epoch: 66000 cost: 0.673550 scoring: 0.595712\n",
      "epoch: 67000 cost: 0.673546 scoring: 0.595712\n",
      "epoch: 68000 cost: 0.673543 scoring: 0.595712\n",
      "epoch: 69000 cost: 0.673539 scoring: 0.595712\n",
      "epoch: 70000 cost: 0.673536 scoring: 0.595712\n",
      "epoch: 71000 cost: 0.673532 scoring: 0.595712\n",
      "epoch: 72000 cost: 0.673529 scoring: 0.595712\n",
      "epoch: 73000 cost: 0.673526 scoring: 0.595712\n",
      "epoch: 74000 cost: 0.673523 scoring: 0.595712\n",
      "epoch: 75000 cost: 0.673520 scoring: 0.595712\n",
      "epoch: 76000 cost: 0.673518 scoring: 0.595712\n",
      "epoch: 77000 cost: 0.673515 scoring: 0.595712\n",
      "epoch: 78000 cost: 0.673512 scoring: 0.595712\n",
      "epoch: 79000 cost: 0.673510 scoring: 0.595712\n",
      "epoch: 80000 cost: 0.673508 scoring: 0.595712\n",
      "epoch: 81000 cost: 0.673505 scoring: 0.595712\n",
      "epoch: 82000 cost: 0.673503 scoring: 0.595712\n",
      "epoch: 83000 cost: 0.673501 scoring: 0.595712\n",
      "epoch: 84000 cost: 0.673499 scoring: 0.595712\n",
      "epoch: 85000 cost: 0.673497 scoring: 0.595712\n",
      "epoch: 86000 cost: 0.673495 scoring: 0.595712\n",
      "epoch: 87000 cost: 0.673493 scoring: 0.595712\n",
      "epoch: 88000 cost: 0.673491 scoring: 0.595712\n",
      "epoch: 89000 cost: 0.673490 scoring: 0.595712\n",
      "epoch: 90000 cost: 0.673488 scoring: 0.595712\n",
      "epoch: 91000 cost: 0.673486 scoring: 0.595712\n",
      "epoch: 92000 cost: 0.673485 scoring: 0.595712\n",
      "epoch: 93000 cost: 0.673483 scoring: 0.595712\n",
      "epoch: 94000 cost: 0.673482 scoring: 0.595712\n",
      "epoch: 95000 cost: 0.673481 scoring: 0.595712\n",
      "epoch: 96000 cost: 0.673479 scoring: 0.595712\n",
      "epoch: 97000 cost: 0.673478 scoring: 0.595712\n",
      "epoch: 98000 cost: 0.673477 scoring: 0.595712\n",
      "epoch: 99000 cost: 0.673475 scoring: 0.595712\n",
      "loop break due to early stoping parameter\n"
     ]
    }
   ],
   "source": [
    "XX = X_train_rfe.reshape((3, -1))\n",
    "YY = y_train.values.reshape((1, -1))\n",
    "\n",
    "history = LogisticRegression(X=XX, Y=YY, maxValue=2, print_val=True, learning_rate=0.00005, beta1=0.9, beta2=0.99,\n",
    "                                                step=1000, early_stoping=100000, epsilon=1e-17, optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
